{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import soundfile\n",
    "import librosa\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_chromagram(waveform, sample_rate):\n",
    "    # STFT computed here explicitly; mel spectrogram and MFCC functions do this under the hood\n",
    "    stft_spectrogram=np.abs(librosa.stft(waveform))\n",
    "    # Produce the chromagram for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
    "    chromagram=np.mean(librosa.feature.chroma_stft(S=stft_spectrogram, sr=sample_rate).T,axis=0)\n",
    "    return chromagram\n",
    "\n",
    "def feature_melspectrogram(waveform, sample_rate, n_mels=128):\n",
    "    # Produce the mel spectrogram for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
    "    # Using 8khz as upper frequency bound should be enough for most speech classification tasks\n",
    "    melspectrogram=np.mean(librosa.feature.melspectrogram(y=waveform, sr=sample_rate, n_mels=n_mels, fmax=8000).T,axis=0)\n",
    "    return melspectrogram\n",
    "\n",
    "def feature_mfcc(waveform, sample_rate, n_mfcc=28):\n",
    "    # Compute the MFCCs for all STFT frames and get the mean of each column of the resulting matrix to create a feature array\n",
    "    # 40 filterbanks = 40 coefficients\n",
    "    mfc_coefficients=np.mean(librosa.feature.mfcc(y=waveform, sr=sample_rate, n_mfcc=n_mfcc).T, axis=0) \n",
    "    return mfc_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chroma = 0\n",
    "n_mels = 0\n",
    "n_mfcc = 42\n",
    "\n",
    "def get_features(file):\n",
    "    # load an individual soundfile\n",
    "     with soundfile.SoundFile(file) as audio:\n",
    "        waveform = audio.read(dtype=\"float32\")\n",
    "        sample_rate = audio.samplerate # 4000\n",
    "        # compute features of soundfile\n",
    "        chromagram = feature_chromagram(waveform, sample_rate)\n",
    "        melspectrogram = feature_melspectrogram(waveform, sample_rate, n_mels)\n",
    "        mfc_coefficients = feature_mfcc(waveform, sample_rate, n_mfcc)\n",
    "\n",
    "        feature_matrix=np.array([])\n",
    "        # use np.hstack to stack our feature arrays horizontally to create a feature matrix\n",
    "        feature_matrix = np.hstack((chromagram, melspectrogram, mfc_coefficients))\n",
    "        \n",
    "        return feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{2530: 0, 9979: 0, 9983: 0, 13918: 0, 14241: 0, 14998: 0, 23625: 0, 24160: 0, 29045: 0, 29378: 0, 31737: 0, 33151: 0, 36327: 0, 38337: 0, 39043: 0, 39403: 0, 39456: 0, 40058: 0, 40798: 0, 40840: 0, 43852: 1, 44514: 0, 45843: 0, 46065: 0, 46532: 1, 46579: 0, 46778: 0, 47002: 0, 49558: 0, 49561: 0, 49562: 0, 49568: 1, 49572: 1, 49574: 0, 49577: 1, 49585: 0, 49595: 0, 49598: 1, 49607: 0, 49610: 0, 49618: 0, 49622: 0, 49627: 0, 49628: 0, 49630: 0, 49631: 1, 49638: 0, 49641: 0, 49653: 1, 49659: 0, 49661: 1, 49669: 0, 49678: 1, 49683: 0, 49687: 0, 49691: 0, 49704: 0, 49712: 0, 49719: 1, 49729: 0, 49735: 0, 49745: 0, 49748: 0, 49751: 0, 49754: 0, 49761: 0, 49776: 0, 49808: 1, 49821: 0, 49823: 0, 49824: 0, 49829: 0, 49832: 1, 49838: 0, 49839: 1, 49842: 0, 49850: 0, 49853: 1, 49854: 0, 49873: 0, 49876: 0, 49896: 1, 49897: 1, 49900: 0, 49930: 1, 49931: 1, 49946: 0, 49952: 1, 49959: 1, 49960: 1, 49963: 0, 49966: 0, 49968: 1, 49969: 1, 49970: 1, 49974: 1, 49978: 0, 49979: 1, 49980: 0, 49983: 1, 49986: 1, 49987: 1, 49988: 1, 49989: 0, 49990: 1, 49993: 1, 49994: 0, 49995: 1, 49998: 1, 49999: 1, 50001: 1, 50004: 1, 50005: 1, 50006: 1, 50007: 1, 50008: 1, 50009: 1, 50012: 0, 50014: 1, 50015: 0, 50017: 1, 50018: 0, 50023: 1, 50026: 1, 50027: 1, 50029: 1, 50030: 0, 50032: 1, 50034: 1, 50037: 1, 50043: 1, 50047: 1, 50048: 1, 50049: 0, 50053: 1, 50054: 0, 50056: 0, 50057: 1, 50061: 0, 50066: 0, 50067: 0, 50070: 0, 50072: 0, 50074: 1, 50075: 1, 50076: 0, 50077: 1, 50078: 0, 50079: 1, 50080: 0, 50085: 1, 50086: 1, 50089: 0, 50092: 1, 50094: 1, 50096: 0, 50099: 1, 50100: 0, 50103: 1, 50104: 1, 50105: 1, 50109: 1, 50111: 0, 50113: 1, 50115: 0, 50116: 1, 50117: 1, 50118: 1, 50119: 1, 50121: 1, 50122: 1, 50123: 0, 50125: 0, 50126: 1, 50127: 0, 50128: 0, 50129: 1, 50133: 1, 50136: 0, 50137: 1, 50138: 1, 50141: 1, 50142: 1, 50143: 1, 50145: 1, 50146: 1, 50149: 0, 50150: 0, 50151: 0, 50152: 0, 50153: 0, 50155: 0, 50159: 0, 50160: 1, 50161: 0, 50164: 1, 50165: 0, 50166: 0, 50168: 1, 50174: 0, 50204: 1, 50206: 0, 50207: 0, 50209: 0, 50210: 1, 50213: 1, 50214: 1, 50216: 0, 50217: 0, 50218: 1, 50219: 0, 50220: 0, 50221: 1, 50222: 1, 50225: 1, 50228: 1, 50229: 0, 50230: 1, 50231: 1, 50233: 0, 50238: 0, 50239: 1, 50241: 0, 50244: 0, 50247: 1, 50248: 0, 50249: 1, 50250: 1, 50251: 1, 50254: 1, 50255: 0, 50256: 1, 50258: 1, 50260: 1, 50261: 0, 50263: 1, 50264: 1, 50271: 1, 50272: 1, 50273: 1, 50275: 1, 50276: 1, 50277: 0, 50278: 1, 50280: 0, 50281: 0, 50284: 1, 50285: 0, 50289: 0, 50291: 0, 50295: 0, 50296: 0, 50297: 0, 50298: 1, 50299: 1, 50300: 0, 50303: 0, 50304: 1, 50306: 0, 50311: 1, 50312: 0, 50314: 0, 50316: 1, 50317: 1, 50318: 0, 50319: 0, 50321: 0, 50323: 0, 50325: 0, 50326: 0, 50327: 1, 50330: 0, 50331: 0, 50332: 0, 50334: 1, 50335: 0, 50336: 1, 50337: 1, 50339: 1, 50341: 0, 50342: 0, 50343: 0, 50345: 1, 50348: 0, 50349: 1, 50350: 0, 50352: 1, 50354: 0, 50359: 0, 50375: 0, 50379: 0, 50384: 1, 50385: 1, 50386: 0, 50388: 1, 50391: 1, 50393: 0, 50619: 0, 50620: 1, 50621: 0, 50624: 0, 50625: 0, 50626: 0, 50628: 0, 50629: 0, 50631: 0, 50635: 1, 50636: 0, 50639: 1, 50640: 0, 50641: 1, 50643: 0, 50644: 1, 50645: 1, 50646: 1, 50647: 1, 50649: 0, 50652: 0, 50654: 1, 50655: 0, 50656: 1, 50657: 1, 50658: 1, 50659: 0, 50661: 1, 50664: 0, 50665: 0, 50667: 1, 50668: 0, 50669: 1, 50671: 0, 50673: 0, 50676: 1, 50677: 0, 50678: 0, 50680: 0, 50685: 0, 50687: 1, 50688: 1, 50689: 0, 50690: 0, 50691: 1, 50693: 0, 50699: 1, 50704: 1, 50707: 1, 50708: 0, 50713: 1, 50715: 1, 50720: 0, 50721: 0, 50722: 1, 50723: 0, 50725: 0, 50727: 0, 50729: 1, 50731: 1, 50732: 0, 50734: 0, 50735: 1, 50736: 0, 50737: 1, 50738: 0, 50739: 0, 50740: 0, 50742: 0, 50743: 1, 50744: 0, 50746: 0, 50747: 1, 50748: 1, 50749: 1, 50751: 1, 50752: 1, 50753: 1, 50754: 1, 50756: 1, 50757: 1, 50758: 1, 50762: 1, 50763: 0, 50766: 1, 50768: 1, 50770: 1, 50771: 0, 50772: 1, 50773: 0, 50774: 0, 50776: 0, 50781: 1, 50782: 0, 50784: 1, 50787: 1, 50788: 1, 50789: 0, 50790: 0, 50793: 1, 50795: 1, 50796: 0, 50797: 0, 50798: 1, 50800: 1, 50802: 1, 50803: 0, 50805: 1, 50807: 0, 50812: 0, 50815: 1, 50818: 0, 50819: 1, 50820: 1, 50822: 1, 50826: 0, 50829: 1, 51064: 1, 51331: 0, 55945: 0, 57700: 0, 57706: 0, 59536: 0, 61117: 0, 61610: 0, 63456: 0, 63581: 0, 64256: 0, 64715: 1, 68175: 0, 68182: 0, 68186: 1, 68194: 0, 68204: 0, 68213: 1, 68219: 0, 68222: 0, 68255: 0, 68260: 1, 68269: 0, 68279: 0, 68292: 0, 68298: 0, 68303: 0, 68306: 0, 68316: 1, 68318: 0, 68327: 0, 68330: 0, 68337: 1, 68347: 0, 68359: 0, 68363: 0, 68368: 0, 68374: 0, 68377: 0, 68379: 0, 68394: 1, 68395: 1, 68404: 1, 68406: 0, 68407: 1, 68412: 1, 68413: 1, 68419: 0, 68423: 0, 68425: 0, 68427: 1, 68431: 1, 68432: 0, 68435: 0, 68436: 1, 68444: 0, 68449: 1, 68456: 0, 68460: 1, 68465: 0, 68470: 1, 68477: 1, 68478: 0, 68482: 0, 68484: 0, 68487: 0, 68498: 0, 68504: 1, 68532: 0, 68545: 0, 68556: 0, 68560: 0, 68567: 0, 68576: 0, 68582: 0, 68624: 0, 68632: 0, 68646: 0, 68659: 0, 68660: 1, 68682: 0, 68698: 0, 68702: 0, 68705: 1, 68708: 0, 68711: 0, 68737: 0, 68738: 0, 68740: 0, 68741: 0, 68752: 1, 68755: 0, 68756: 1, 68757: 1, 68796: 1, 68827: 0, 68831: 0, 68849: 0, 68857: 0, 68861: 0, 68864: 0, 68874: 0, 68886: 1, 68887: 0, 68888: 0, 68895: 0, 68901: 1, 68908: 0, 68909: 0, 68952: 0, 69060: 0, 69066: 0, 69067: 1, 69068: 0, 69079: 0, 69093: 0, 69095: 0, 69096: 0, 69106: 0, 69112: 0, 69120: 0, 69125: 0, 69129: 0, 69141: 0, 69144: 1, 69147: 0, 69152: 0, 69155: 0, 69159: 0, 69161: 1, 69174: 0, 69176: 0, 69188: 0, 70280: 0, 72283: 0, 72288: 0, 73316: 0, 73497: 0, 74417: 0, 74420: 0, 75440: 0, 76240: 0, 76758: 0, 77373: 0, 78280: 0, 78582: 0, 78592: 0, 80348: 0, 81035: 0, 81297: 1, 81501: 0, 81638: 0, 82275: 1, 83094: 0, 84687: 1, 84688: 1, 84689: 1, 84690: 0, 84692: 0, 84693: 0, 84695: 1, 84696: 0, 84697: 1, 84699: 0, 84702: 0, 84704: 0, 84706: 0, 84708: 0, 84709: 1, 84710: 0, 84711: 1, 84713: 0, 84714: 0, 84716: 0, 84718: 0, 84720: 0, 84721: 0, 84724: 1, 84725: 1, 84727: 1, 84730: 1, 84731: 1, 84732: 0, 84733: 0, 84734: 0, 84735: 1, 84736: 0, 84738: 1, 84740: 1, 84742: 1, 84743: 1, 84746: 1, 84747: 1, 84749: 1, 84750: 1, 84751: 0, 84753: 1, 84754: 1, 84755: 1, 84758: 1, 84760: 1, 84761: 1, 84762: 1, 84764: 1, 84765: 1, 84768: 1, 84769: 1, 84775: 1, 84776: 1, 84778: 1, 84779: 1, 84780: 1, 84784: 1, 84785: 1, 84786: 1, 84790: 1, 84793: 1, 84796: 0, 84798: 1, 84799: 1, 84802: 1, 84803: 0, 84804: 1, 84805: 1, 84807: 1, 84808: 1, 84809: 1, 84813: 1, 84814: 1, 84815: 1, 84822: 1, 84823: 1, 84824: 0, 84826: 1, 84829: 1, 84831: 1, 84834: 0, 84835: 1, 84837: 0, 84838: 1, 84839: 1, 84840: 0, 84851: 0, 84852: 1, 84853: 0, 84854: 0, 84855: 1, 84856: 0, 84857: 0, 84859: 1, 84861: 1, 84863: 0, 84864: 1, 84865: 0, 84866: 1, 84868: 1, 84870: 0, 84874: 1, 84875: 1, 84876: 0, 84877: 1, 84878: 1, 84879: 1, 84881: 1, 84882: 0, 84883: 1, 84884: 1, 84885: 0, 84886: 1, 84887: 1, 84890: 1, 84892: 1, 84893: 1, 84894: 1, 84896: 1, 84900: 1, 84912: 1, 84917: 1, 84918: 1, 84919: 1, 84920: 1, 84921: 1, 84922: 1, 84923: 1, 84928: 1, 84930: 1, 84931: 0, 84933: 1, 84934: 0, 84935: 1, 84936: 1, 84937: 1, 84939: 1, 84942: 1, 84945: 1, 84946: 1, 84947: 0, 84949: 0, 84950: 0, 84952: 1, 84957: 1, 84960: 1, 84961: 1, 84962: 1, 84965: 0, 84966: 1, 84969: 0, 84970: 1, 84971: 1, 84973: 1, 84974: 1, 84976: 1, 84977: 1, 84978: 1, 84982: 1, 84983: 1, 84984: 0, 84985: 1, 84986: 1, 84987: 1, 84988: 1, 84990: 1, 84991: 1, 84992: 1, 84993: 0, 84994: 0, 84995: 1, 84996: 0, 85000: 1, 85002: 0, 85004: 0, 85010: 0, 85011: 1, 85012: 0, 85018: 0, 85019: 0, 85020: 0, 85023: 1, 85024: 0, 85026: 0, 85027: 1, 85028: 1, 85029: 1, 85030: 0, 85031: 0, 85033: 1, 85034: 0, 85035: 1, 85036: 0, 85037: 0, 85038: 0, 85042: 0, 85043: 1, 85044: 1, 85046: 1, 85048: 1, 85052: 0, 85053: 1, 85055: 0, 85057: 0, 85062: 1, 85063: 1, 85064: 1, 85066: 1, 85069: 0, 85075: 1, 85076: 1, 85077: 1, 85079: 0, 85080: 0, 85081: 0, 85084: 1, 85086: 1, 85087: 0, 85090: 1, 85091: 1, 85093: 1, 85094: 1, 85096: 1, 85099: 1, 85100: 1, 85102: 1, 85103: 1, 85105: 1, 85108: 0, 85109: 1, 85110: 1, 85112: 1, 85113: 1, 85114: 1, 85115: 1, 85116: 0, 85118: 1, 85119: 0, 85121: 1, 85122: 1, 85123: 1, 85124: 1, 85127: 0, 85128: 1, 85131: 1, 85132: 0, 85133: 0, 85134: 1, 85135: 1, 85136: 1, 85139: 0, 85140: 1, 85143: 1, 85144: 0, 85145: 0, 85147: 1, 85148: 1, 85150: 1, 85151: 1, 85152: 1, 85153: 1, 85154: 1, 85155: 1, 85157: 1, 85159: 1, 85161: 0, 85162: 0, 85163: 0, 85164: 1, 85165: 0, 85166: 1, 85167: 1, 85168: 1, 85169: 0, 85172: 0, 85174: 0, 85175: 1, 85176: 1, 85180: 1, 85181: 1, 85182: 1, 85184: 1, 85186: 1, 85192: 0, 85196: 1, 85197: 1, 85198: 0, 85199: 0, 85202: 0, 85203: 1, 85207: 0, 85210: 1, 85212: 1, 85213: 0, 85214: 1, 85216: 1, 85217: 1, 85219: 0, 85222: 1, 85225: 0, 85226: 1, 85227: 0, 85229: 1, 85230: 1, 85234: 1, 85235: 1, 85236: 0, 85239: 0, 85240: 1, 85241: 1, 85242: 1, 85243: 0, 85244: 0, 85245: 1, 85246: 1, 85247: 0, 85249: 1, 85250: 1, 85252: 1, 85253: 0, 85258: 1, 85259: 0, 85261: 0, 85262: 0, 85264: 1, 85265: 1, 85269: 1, 85270: 0, 85276: 0, 85277: 1, 85278: 0, 85279: 0, 85282: 1, 85285: 1, 85286: 1, 85287: 1, 85288: 1, 85293: 0, 85294: 0, 85296: 0, 85299: 1, 85300: 1, 85301: 1, 85305: 0, 85306: 0, 85308: 1, 85312: 1, 85313: 1, 85315: 0, 85316: 1, 85317: 1, 85319: 0, 85321: 1, 85322: 1, 85323: 1, 85326: 1, 85327: 1, 85328: 1, 85329: 1, 85331: 1, 85332: 1, 85334: 1, 85335: 1, 85336: 1, 85337: 1, 85338: 1, 85339: 1, 85340: 1, 85341: 1, 85343: 0, 85345: 1, 85349: 1}\n"
     ]
    }
   ],
   "source": [
    "dataset_info = pd.read_csv('assets/the-circor-digiscope-phonocardiogram-dataset-1.0.3/training_data.csv')\n",
    "\n",
    "outcome_mapping = {'Normal': 1, 'Abnormal': 0}\n",
    "dataset_info['Mapped_Outcome'] = dataset_info['Outcome'].map(outcome_mapping)\n",
    "y_dict = dict(zip(dataset_info['Patient ID'], dataset_info['Mapped_Outcome']))\n",
    "\n",
    "print(y_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13918_AV.wav',\n",
       " '13918_MV.wav',\n",
       " '13918_PV.wav',\n",
       " '13918_TV.wav',\n",
       " '14241_AV.wav']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_files_by_keywords_and_extension(folder_path, keywords, extension):\n",
    "    filtered_files = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if any(keyword in filename for keyword in keywords) and filename.endswith(extension):\n",
    "            filtered_files.append(filename)\n",
    "    return filtered_files\n",
    "\n",
    "folder_path = 'assets/the-circor-digiscope-phonocardiogram-dataset-1.0.3/training_data'\n",
    "keywords = ['TV','AV','PV','MV']\n",
    "extension = '.wav'\n",
    "\n",
    "filtered_files = filter_files_by_keywords_and_extension(folder_path, keywords, extension)\n",
    "filtered_files[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 3159/3159 audio samples \n"
     ]
    }
   ],
   "source": [
    "def load_data(filtered_files):\n",
    "    X, y = [], []\n",
    "    count = 0\n",
    "    for file in filtered_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        features = get_features(file_path) \n",
    "        file_number = int(re.match(r'^([^_]*)', file)[1])\n",
    "        label = y_dict[file_number]\n",
    "        X.append(features)\n",
    "        y.append(label)\n",
    "        count += 1\n",
    "        print('\\r' + f'Processed {count}/{len(filtered_files)} audio samples', end=' ')\n",
    "    print()  # Print a newline after the loop completes\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "features, labels = load_data(filtered_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(features).to_csv(\"./assets/feature(withoutOpenSmile).csv\", header=False, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many samples in total:  3159\n",
      "How many samples are Normal:  1632\n"
     ]
    }
   ],
   "source": [
    "print('How many samples in total: ', len(labels))\n",
    "\n",
    "print('How many samples are Normal: ', sum(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Audio samples represented: 3159\n",
      "Numerical features extracted per sample: 54\n",
      "n_chroma 0\n",
      "n_mels 0\n",
      "mfcc 42\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nAudio samples represented: {features.shape[0]}')\n",
    "print(f'Numerical features extracted per sample: {features.shape[1]}')\n",
    "features_df = pd.DataFrame(features) # make it pretty for display\n",
    "features_df\n",
    "\n",
    "\n",
    "\n",
    "print('n_chroma', n_chroma)\n",
    "print('n_mels',n_mels)\n",
    "print('mfcc', n_mfcc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Chromagram features:           min = 0.164,     max = 0.997,     mean = 0.808,     deviation = 0.076\n",
      "\n",
      "0 Mel Spectrogram features:     min = nan,     max = nan,     mean = nan,     deviation = nan\n",
      "\n",
      "42 MFCC features:                 min = -344.715,    max = 197.198,    mean = -1.271,    deviation = 37.698\n"
     ]
    }
   ],
   "source": [
    "# We would usually use df.describe(), but it provides a bit of a mess of information we don't need at the moment.\n",
    "def print_features(df):\n",
    "    # Check chromagram feature values\n",
    "    features_df_chromagram = df.loc[:,:11]\n",
    "    chroma_min = features_df_chromagram.min().min()\n",
    "    chroma_max = features_df_chromagram.max().max()\n",
    "    # stack all features into a single series so we don't get a mean of means or stdev of stdevs\n",
    "    chroma_mean = features_df_chromagram.stack().mean()\n",
    "    chroma_stdev = features_df_chromagram.stack().std()\n",
    "    print(f'{n_chroma} Chromagram features:       \\\n",
    "    min = {chroma_min:.3f}, \\\n",
    "    max = {chroma_max:.3f}, \\\n",
    "    mean = {chroma_mean:.3f}, \\\n",
    "    deviation = {chroma_stdev:.3f}') \n",
    "\n",
    "    # Check mel spectrogram feature values\n",
    "    features_df_melspectrogram = df.loc[:,n_chroma:n_chroma+n_mels-1]\n",
    "    mel_min = features_df_melspectrogram.min().min()\n",
    "    mel_max = features_df_melspectrogram.max().max()\n",
    "    # stack all features into a single series so we don't get a mean of means or stdev of stdevs\n",
    "    mel_mean = features_df_melspectrogram.stack().mean()\n",
    "    mel_stdev = features_df_melspectrogram.stack().std()\n",
    "    print(f'\\n{n_mels} Mel Spectrogram features: \\\n",
    "    min = {mel_min:.3f}, \\\n",
    "    max = {mel_max:.3f}, \\\n",
    "    mean = {mel_mean:.3f}, \\\n",
    "    deviation = {mel_stdev:.3f}')\n",
    "\n",
    "    # Check MFCC feature values\n",
    "    features_df_mfcc = df.loc[:,n_chroma+n_mels:n_chroma+n_mels+n_mfcc-1]\n",
    "    mfcc_min = features_df_mfcc.min().min()\n",
    "    mfcc_max = features_df_mfcc.max().max()\n",
    "    # stack all features into a single series so we don't get a mean of means or stdev of stdevs\n",
    "    mfcc_mean = features_df_mfcc.stack().mean()\n",
    "    mfcc_stdev = features_df_mfcc.stack().std()\n",
    "    print(f'\\n{n_mfcc} MFCC features:             \\\n",
    "    min = {mfcc_min:.3f},\\\n",
    "    max = {mfcc_max:.3f},\\\n",
    "    mean = {mfcc_mean:.3f},\\\n",
    "    deviation = {mfcc_stdev:.3f}')\n",
    "    \n",
    "print_features(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# keep our unscaled features just in case we need to process them alternatively\n",
    "features_scaled = features \n",
    "features_scaled = scaler.fit_transform(features_scaled)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# keep our unscaled features just in case we need to process them alternatively\n",
    "features_minmax = features\n",
    "features_minmax = scaler.fit_transform(features_minmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mStandard Scaling:\n",
      "\u001b[0m\n",
      "0 Chromagram features:           min = -12.385,     max = 3.475,     mean = 0.000,     deviation = 1.000\n",
      "\n",
      "0 Mel Spectrogram features:     min = nan,     max = nan,     mean = nan,     deviation = nan\n",
      "\n",
      "42 MFCC features:                 min = -12.385,    max = 10.532,    mean = -0.000,    deviation = 1.000\n",
      "\n",
      "\n",
      "\u001b[1mMinMax Scaling:\n",
      "\u001b[0m\n",
      "0 Chromagram features:           min = 0.000,     max = 1.000,     mean = 0.784,     deviation = 0.105\n",
      "\n",
      "0 Mel Spectrogram features:     min = nan,     max = nan,     mean = nan,     deviation = nan\n",
      "\n",
      "42 MFCC features:                 min = 0.000,    max = 1.000,    mean = 0.579,    deviation = 0.189\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m'+'Standard Scaling:\\n'+'\\033[0m')\n",
    "features_scaled_df = pd.DataFrame(features_scaled)\n",
    "print_features(features_scaled_df)\n",
    "\n",
    "print('\\n\\n\\033[1m'+'MinMax Scaling:\\n'+'\\033[0m')\n",
    "features_minmax_df = pd.DataFrame(features_minmax)\n",
    "print_features(features_minmax_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "############ Unscaled test/train set #############\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, \n",
    "    labels, \n",
    "    test_size=0.2, \n",
    "    random_state=69\n",
    ")\n",
    "\n",
    "############ Standard Scaled test/train set ###########\n",
    "# The labels/classes (y_train, y_test) never change, keep old values \n",
    "X_train_scaled, X_test_scaled, _, _ = train_test_split(\n",
    "    features_scaled, \n",
    "    labels, \n",
    "    test_size=0.2, \n",
    "    random_state=69\n",
    ")\n",
    "\n",
    "############# MinMax Scaled test/train set ###############\n",
    "# The labels/classes (y_train, y_test) never change, keep old values \n",
    "X_train_minmax, X_test_minmax, _, _ = train_test_split(\n",
    "    features_minmax, \n",
    "    labels, \n",
    "    test_size=0.2, \n",
    "    random_state=69\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model(input_shape, num_classes, model_type='cnn'):\n",
    "    model = Sequential()\n",
    "    \n",
    "    if model_type == 'cnn':\n",
    "        model.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(Flatten())\n",
    "    elif model_type == 'lstm':\n",
    "        model.add(LSTM(64, return_sequences=True, input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LSTM(128))\n",
    "    else:\n",
    "        raise ValueError(\"model_type should be 'cnn' or 'lstm'\")\n",
    "    \n",
    "    # Common part for both architectures\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    # Output layer\n",
    "    if num_classes == 2:\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    elif num_classes > 2:\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        raise ValueError(\"num_classes should be >= 2\")\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\songy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1408</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">180,352</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m384\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m24,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1408\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m180,352\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">206,337</span> (806.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m206,337\u001b[0m (806.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">205,953</span> (804.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m205,953\u001b[0m (804.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_shape = (54, 1)  # Example input shape\n",
    "num_classes = 2  # Binary classification\n",
    "model = build_model(input_shape, num_classes, model_type='cnn')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_new, X_val, y_train_new, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "X_train_scaled_new, X_val_scaled, y_train_new, y_val = train_test_split(\n",
    "    X_train_scaled, y_train, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.5248 - loss: 1.1820 - val_accuracy: 0.5079 - val_loss: 0.7976\n",
      "Epoch 2/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5407 - loss: 0.6877 - val_accuracy: 0.6265 - val_loss: 0.6620\n",
      "Epoch 3/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5753 - loss: 0.6771 - val_accuracy: 0.6067 - val_loss: 0.6645\n",
      "Epoch 4/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5693 - loss: 0.6769 - val_accuracy: 0.5830 - val_loss: 0.6723\n",
      "Epoch 5/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5835 - loss: 0.6767 - val_accuracy: 0.5711 - val_loss: 0.6710\n",
      "Epoch 6/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5827 - loss: 0.6700 - val_accuracy: 0.6067 - val_loss: 0.6613\n",
      "Epoch 7/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5898 - loss: 0.6700 - val_accuracy: 0.5711 - val_loss: 0.6807\n",
      "Epoch 8/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5729 - loss: 0.6715 - val_accuracy: 0.5850 - val_loss: 0.6695\n",
      "Epoch 9/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5702 - loss: 0.6789 - val_accuracy: 0.5929 - val_loss: 0.6637\n",
      "Epoch 10/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6029 - loss: 0.6722 - val_accuracy: 0.5889 - val_loss: 0.6681\n",
      "Epoch 11/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5663 - loss: 0.6738 - val_accuracy: 0.6225 - val_loss: 0.6587\n",
      "Epoch 12/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5997 - loss: 0.6617 - val_accuracy: 0.6206 - val_loss: 0.6586\n",
      "Epoch 13/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5873 - loss: 0.6699 - val_accuracy: 0.6107 - val_loss: 0.6602\n",
      "Epoch 14/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5913 - loss: 0.6649 - val_accuracy: 0.6107 - val_loss: 0.6562\n",
      "Epoch 15/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5984 - loss: 0.6620 - val_accuracy: 0.6186 - val_loss: 0.6517\n",
      "Epoch 16/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5963 - loss: 0.6686 - val_accuracy: 0.6186 - val_loss: 0.6561\n",
      "Epoch 17/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5857 - loss: 0.6649 - val_accuracy: 0.5929 - val_loss: 0.6636\n",
      "Epoch 18/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5903 - loss: 0.6673 - val_accuracy: 0.6166 - val_loss: 0.6574\n",
      "Epoch 19/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5790 - loss: 0.6668 - val_accuracy: 0.6225 - val_loss: 0.6629\n",
      "Epoch 20/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6091 - loss: 0.6545 - val_accuracy: 0.5988 - val_loss: 0.6655\n",
      "Epoch 21/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5766 - loss: 0.6661 - val_accuracy: 0.5909 - val_loss: 0.6602\n",
      "Epoch 22/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6054 - loss: 0.6607 - val_accuracy: 0.5988 - val_loss: 0.6617\n",
      "Epoch 23/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5921 - loss: 0.6667 - val_accuracy: 0.5929 - val_loss: 0.6597\n",
      "Epoch 24/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6067 - loss: 0.6560 - val_accuracy: 0.5830 - val_loss: 0.6623\n",
      "Epoch 25/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5957 - loss: 0.6558 - val_accuracy: 0.6107 - val_loss: 0.6560\n",
      "Epoch 26/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5901 - loss: 0.6642 - val_accuracy: 0.6047 - val_loss: 0.6626\n",
      "Epoch 27/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5869 - loss: 0.6572 - val_accuracy: 0.5791 - val_loss: 0.6612\n",
      "Epoch 28/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5777 - loss: 0.6644 - val_accuracy: 0.5652 - val_loss: 0.6666\n",
      "Epoch 29/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5980 - loss: 0.6565 - val_accuracy: 0.6166 - val_loss: 0.6548\n",
      "Epoch 30/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5892 - loss: 0.6628 - val_accuracy: 0.5731 - val_loss: 0.6681\n",
      "Epoch 31/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5762 - loss: 0.6638 - val_accuracy: 0.6265 - val_loss: 0.6548\n",
      "Epoch 32/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5895 - loss: 0.6508 - val_accuracy: 0.6126 - val_loss: 0.6526\n",
      "Epoch 33/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5959 - loss: 0.6590 - val_accuracy: 0.5988 - val_loss: 0.6606\n",
      "Epoch 34/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6078 - loss: 0.6527 - val_accuracy: 0.5929 - val_loss: 0.6586\n",
      "Epoch 35/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5823 - loss: 0.6515 - val_accuracy: 0.6008 - val_loss: 0.6691\n",
      "Epoch 36/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5860 - loss: 0.6651 - val_accuracy: 0.6047 - val_loss: 0.6605\n",
      "Epoch 37/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6235 - loss: 0.6439 - val_accuracy: 0.6107 - val_loss: 0.6586\n",
      "Epoch 38/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5892 - loss: 0.6539 - val_accuracy: 0.6146 - val_loss: 0.6544\n",
      "Epoch 39/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6144 - loss: 0.6405 - val_accuracy: 0.5988 - val_loss: 0.6618\n",
      "Epoch 40/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5996 - loss: 0.6418 - val_accuracy: 0.6028 - val_loss: 0.6566\n",
      "Epoch 41/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5899 - loss: 0.6534 - val_accuracy: 0.6186 - val_loss: 0.6653\n",
      "Epoch 42/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5939 - loss: 0.6560 - val_accuracy: 0.6166 - val_loss: 0.6599\n",
      "Epoch 43/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6288 - loss: 0.6381 - val_accuracy: 0.5672 - val_loss: 0.6641\n",
      "Epoch 44/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6082 - loss: 0.6470 - val_accuracy: 0.5870 - val_loss: 0.6683\n",
      "Epoch 45/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5842 - loss: 0.6598 - val_accuracy: 0.5909 - val_loss: 0.6572\n",
      "Epoch 46/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6003 - loss: 0.6467 - val_accuracy: 0.6186 - val_loss: 0.6539\n",
      "Epoch 47/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6270 - loss: 0.6341 - val_accuracy: 0.5889 - val_loss: 0.6625\n",
      "Epoch 48/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6134 - loss: 0.6352 - val_accuracy: 0.5771 - val_loss: 0.6755\n",
      "Epoch 49/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5970 - loss: 0.6428 - val_accuracy: 0.5929 - val_loss: 0.6633\n",
      "Epoch 50/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6261 - loss: 0.6371 - val_accuracy: 0.6008 - val_loss: 0.6597\n",
      "Epoch 51/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6055 - loss: 0.6378 - val_accuracy: 0.6028 - val_loss: 0.6629\n",
      "Epoch 52/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6031 - loss: 0.6364 - val_accuracy: 0.5810 - val_loss: 0.6681\n",
      "Epoch 53/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6250 - loss: 0.6248 - val_accuracy: 0.5949 - val_loss: 0.6693\n",
      "Epoch 54/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6170 - loss: 0.6308 - val_accuracy: 0.5889 - val_loss: 0.6590\n",
      "Epoch 55/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6265 - loss: 0.6250 - val_accuracy: 0.5909 - val_loss: 0.6679\n",
      "Epoch 56/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6057 - loss: 0.6388 - val_accuracy: 0.6008 - val_loss: 0.6614\n",
      "Epoch 57/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6402 - loss: 0.6292 - val_accuracy: 0.5889 - val_loss: 0.6748\n",
      "Epoch 58/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6159 - loss: 0.6352 - val_accuracy: 0.6008 - val_loss: 0.6742\n",
      "Epoch 59/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6339 - loss: 0.6291 - val_accuracy: 0.5949 - val_loss: 0.6694\n",
      "Epoch 60/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6323 - loss: 0.6254 - val_accuracy: 0.5870 - val_loss: 0.6698\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x219d864f890>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_new, y_train_new, epochs=60, batch_size=32, validation_data=(X_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Precision: 0.6065573770491803\n",
      "Recall: 0.6416184971098265\n",
      "F1 Score: 0.6235955056179775\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred).astype(int)  # Convert probabilities to binary labels\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5442 - loss: 3.5669 - val_accuracy: 0.4921 - val_loss: 0.8071\n",
      "Epoch 2/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5904 - loss: 0.8250 - val_accuracy: 0.4921 - val_loss: 0.7220\n",
      "Epoch 3/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6459 - loss: 0.6382 - val_accuracy: 0.5059 - val_loss: 0.6937\n",
      "Epoch 4/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6756 - loss: 0.6160 - val_accuracy: 0.4980 - val_loss: 0.7147\n",
      "Epoch 5/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6818 - loss: 0.5772 - val_accuracy: 0.5040 - val_loss: 0.7416\n",
      "Epoch 6/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7116 - loss: 0.5527 - val_accuracy: 0.5178 - val_loss: 0.7429\n",
      "Epoch 7/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7264 - loss: 0.5334 - val_accuracy: 0.5237 - val_loss: 0.7280\n",
      "Epoch 8/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7265 - loss: 0.5287 - val_accuracy: 0.5415 - val_loss: 0.7382\n",
      "Epoch 9/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7392 - loss: 0.5116 - val_accuracy: 0.5534 - val_loss: 0.7207\n",
      "Epoch 10/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7284 - loss: 0.5075 - val_accuracy: 0.5672 - val_loss: 0.7245\n",
      "Epoch 11/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7632 - loss: 0.4645 - val_accuracy: 0.5771 - val_loss: 0.7266\n",
      "Epoch 12/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7876 - loss: 0.4372 - val_accuracy: 0.6008 - val_loss: 0.7341\n",
      "Epoch 13/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7782 - loss: 0.4422 - val_accuracy: 0.5791 - val_loss: 0.7504\n",
      "Epoch 14/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8143 - loss: 0.4232 - val_accuracy: 0.6285 - val_loss: 0.7364\n",
      "Epoch 15/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8345 - loss: 0.3736 - val_accuracy: 0.6304 - val_loss: 0.7777\n",
      "Epoch 16/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8144 - loss: 0.4005 - val_accuracy: 0.6225 - val_loss: 0.7695\n",
      "Epoch 17/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8340 - loss: 0.3589 - val_accuracy: 0.6166 - val_loss: 0.8055\n",
      "Epoch 18/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8351 - loss: 0.3562 - val_accuracy: 0.6067 - val_loss: 0.8443\n",
      "Epoch 19/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8346 - loss: 0.3433 - val_accuracy: 0.6067 - val_loss: 0.8266\n",
      "Epoch 20/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8687 - loss: 0.3203 - val_accuracy: 0.6206 - val_loss: 0.8450\n",
      "Epoch 21/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8672 - loss: 0.3106 - val_accuracy: 0.6126 - val_loss: 0.8767\n",
      "Epoch 22/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8613 - loss: 0.3035 - val_accuracy: 0.6107 - val_loss: 0.9041\n",
      "Epoch 23/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8751 - loss: 0.2806 - val_accuracy: 0.6225 - val_loss: 0.9146\n",
      "Epoch 24/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8795 - loss: 0.2769 - val_accuracy: 0.6126 - val_loss: 0.9208\n",
      "Epoch 25/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8814 - loss: 0.2733 - val_accuracy: 0.6107 - val_loss: 0.9740\n",
      "Epoch 26/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8955 - loss: 0.2541 - val_accuracy: 0.6008 - val_loss: 0.9590\n",
      "Epoch 27/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9169 - loss: 0.2234 - val_accuracy: 0.6206 - val_loss: 0.9371\n",
      "Epoch 28/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8954 - loss: 0.2410 - val_accuracy: 0.6067 - val_loss: 1.0210\n",
      "Epoch 29/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8967 - loss: 0.2395 - val_accuracy: 0.6225 - val_loss: 1.0451\n",
      "Epoch 30/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9015 - loss: 0.2228 - val_accuracy: 0.6047 - val_loss: 1.0269\n",
      "Epoch 31/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8993 - loss: 0.2334 - val_accuracy: 0.6383 - val_loss: 1.0484\n",
      "Epoch 32/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9118 - loss: 0.2159 - val_accuracy: 0.6304 - val_loss: 1.0083\n",
      "Epoch 33/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9157 - loss: 0.1885 - val_accuracy: 0.6383 - val_loss: 1.0651\n",
      "Epoch 34/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9355 - loss: 0.1816 - val_accuracy: 0.6245 - val_loss: 1.0773\n",
      "Epoch 35/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9327 - loss: 0.1736 - val_accuracy: 0.6146 - val_loss: 1.1036\n",
      "Epoch 36/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9328 - loss: 0.1748 - val_accuracy: 0.6067 - val_loss: 1.1067\n",
      "Epoch 37/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9332 - loss: 0.1603 - val_accuracy: 0.6107 - val_loss: 1.1261\n",
      "Epoch 38/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9434 - loss: 0.1508 - val_accuracy: 0.6047 - val_loss: 1.1803\n",
      "Epoch 39/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9372 - loss: 0.1637 - val_accuracy: 0.6008 - val_loss: 1.1757\n",
      "Epoch 40/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9463 - loss: 0.1577 - val_accuracy: 0.5988 - val_loss: 1.1146\n",
      "Epoch 41/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9205 - loss: 0.1943 - val_accuracy: 0.6186 - val_loss: 1.1671\n",
      "Epoch 42/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9371 - loss: 0.1594 - val_accuracy: 0.6206 - val_loss: 1.1869\n",
      "Epoch 43/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9506 - loss: 0.1384 - val_accuracy: 0.6206 - val_loss: 1.2010\n",
      "Epoch 44/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9348 - loss: 0.1588 - val_accuracy: 0.6126 - val_loss: 1.2612\n",
      "Epoch 45/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9402 - loss: 0.1409 - val_accuracy: 0.6107 - val_loss: 1.2505\n",
      "Epoch 46/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9347 - loss: 0.1609 - val_accuracy: 0.6304 - val_loss: 1.2122\n",
      "Epoch 47/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9472 - loss: 0.1356 - val_accuracy: 0.6225 - val_loss: 1.2251\n",
      "Epoch 48/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9479 - loss: 0.1278 - val_accuracy: 0.6206 - val_loss: 1.2611\n",
      "Epoch 49/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9449 - loss: 0.1448 - val_accuracy: 0.6126 - val_loss: 1.2498\n",
      "Epoch 50/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9503 - loss: 0.1313 - val_accuracy: 0.6008 - val_loss: 1.3082\n",
      "Epoch 51/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9443 - loss: 0.1297 - val_accuracy: 0.6107 - val_loss: 1.3108\n",
      "Epoch 52/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9577 - loss: 0.1130 - val_accuracy: 0.6225 - val_loss: 1.3858\n",
      "Epoch 53/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9400 - loss: 0.1456 - val_accuracy: 0.6107 - val_loss: 1.3099\n",
      "Epoch 54/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9505 - loss: 0.1299 - val_accuracy: 0.6383 - val_loss: 1.2773\n",
      "Epoch 55/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9482 - loss: 0.1351 - val_accuracy: 0.6304 - val_loss: 1.3231\n",
      "Epoch 56/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9532 - loss: 0.1274 - val_accuracy: 0.6107 - val_loss: 1.3759\n",
      "Epoch 57/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9514 - loss: 0.1344 - val_accuracy: 0.6008 - val_loss: 1.3416\n",
      "Epoch 58/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9643 - loss: 0.1118 - val_accuracy: 0.6126 - val_loss: 1.3778\n",
      "Epoch 59/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9596 - loss: 0.0933 - val_accuracy: 0.6225 - val_loss: 1.4361\n",
      "Epoch 60/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9615 - loss: 0.0979 - val_accuracy: 0.6008 - val_loss: 1.4735\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x219ea1a8500>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_scaled_new, y_train_new, epochs=60, batch_size=32, validation_data=(X_val_scaled, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Precision: 0.5630252100840336\n",
      "Recall: 0.9682080924855492\n",
      "F1 Score: 0.7120085015940489\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred).astype(int)  # Convert probabilities to binary labels\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\songy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">54</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)       │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling1d               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling1D</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,621,952</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">655,616</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">81,984</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │        <span style=\"color: #00af00; text-decoration-color: #00af00\">66,560</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m54\u001b[0m, \u001b[38;5;34m1024\u001b[0m)       │         \u001b[38;5;34m8,192\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ average_pooling1d               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m1024\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mAveragePooling1D\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │     \u001b[38;5;34m2,621,952\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m512\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │       \u001b[38;5;34m655,616\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │        \u001b[38;5;34m81,984\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_4 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │        \u001b[38;5;34m66,560\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m131,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,575,873</span> (13.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,575,873\u001b[0m (13.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,575,873</span> (13.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,575,873\u001b[0m (13.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "\n",
    "# First Convolutional Block\n",
    "model.add(Conv1D(filters=1024, kernel_size=7, strides=1, padding='same', activation='relu', input_shape=(54, 1))) # Adjust input_shape based on your data\n",
    "model.add(AveragePooling1D(pool_size=5, strides=2, padding='same'))\n",
    "\n",
    "# Second Convolutional Block\n",
    "model.add(Conv1D(filters=512, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n",
    "\n",
    "# Third Convolutional Block\n",
    "model.add(Conv1D(filters=256, kernel_size=5, strides=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n",
    "\n",
    "# Fourth Convolutional Block\n",
    "model.add(Conv1D(filters=64, kernel_size=5, strides=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides=2, padding='same'))\n",
    "\n",
    "# Flatten the output of the convolutional layers\n",
    "model.add(Flatten())\n",
    "\n",
    "# Fully Connected Layers\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Assuming a dropout is desired for regularization\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Assuming a dropout is desired for regularization\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))  # Assuming a dropout is desired for regularization\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for 3 classes\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model Summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6222 - loss: 0.6141 - val_accuracy: 0.6107 - val_loss: 0.6714\n",
      "Epoch 2/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6275 - loss: 0.6224 - val_accuracy: 0.5889 - val_loss: 0.6706\n",
      "Epoch 3/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6511 - loss: 0.6014 - val_accuracy: 0.5889 - val_loss: 0.6704\n",
      "Epoch 4/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6446 - loss: 0.6109 - val_accuracy: 0.5988 - val_loss: 0.6651\n",
      "Epoch 5/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6351 - loss: 0.6191 - val_accuracy: 0.5830 - val_loss: 0.6768\n",
      "Epoch 6/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6546 - loss: 0.6104 - val_accuracy: 0.6107 - val_loss: 0.6661\n",
      "Epoch 7/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6474 - loss: 0.6090 - val_accuracy: 0.5830 - val_loss: 0.6756\n",
      "Epoch 8/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6573 - loss: 0.6000 - val_accuracy: 0.6047 - val_loss: 0.6745\n",
      "Epoch 9/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6520 - loss: 0.6010 - val_accuracy: 0.5909 - val_loss: 0.6767\n",
      "Epoch 10/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6445 - loss: 0.5968 - val_accuracy: 0.5731 - val_loss: 0.6781\n",
      "Epoch 11/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6213 - loss: 0.6137 - val_accuracy: 0.5949 - val_loss: 0.6749\n",
      "Epoch 12/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6574 - loss: 0.5962 - val_accuracy: 0.5889 - val_loss: 0.6721\n",
      "Epoch 13/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6180 - loss: 0.6118 - val_accuracy: 0.5889 - val_loss: 0.6747\n",
      "Epoch 14/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6557 - loss: 0.5938 - val_accuracy: 0.5830 - val_loss: 0.6874\n",
      "Epoch 15/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6348 - loss: 0.6067 - val_accuracy: 0.5968 - val_loss: 0.6711\n",
      "Epoch 16/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6580 - loss: 0.5956 - val_accuracy: 0.6146 - val_loss: 0.6691\n",
      "Epoch 17/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6442 - loss: 0.5940 - val_accuracy: 0.5613 - val_loss: 0.6794\n",
      "Epoch 18/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6560 - loss: 0.5955 - val_accuracy: 0.5909 - val_loss: 0.6740\n",
      "Epoch 19/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6444 - loss: 0.6140 - val_accuracy: 0.6107 - val_loss: 0.6796\n",
      "Epoch 20/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6491 - loss: 0.5927 - val_accuracy: 0.5909 - val_loss: 0.6897\n",
      "Epoch 21/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6737 - loss: 0.5907 - val_accuracy: 0.5870 - val_loss: 0.6851\n",
      "Epoch 22/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6477 - loss: 0.5974 - val_accuracy: 0.5830 - val_loss: 0.6773\n",
      "Epoch 23/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6508 - loss: 0.5894 - val_accuracy: 0.6047 - val_loss: 0.6790\n",
      "Epoch 24/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6684 - loss: 0.5875 - val_accuracy: 0.6087 - val_loss: 0.6788\n",
      "Epoch 25/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6349 - loss: 0.5882 - val_accuracy: 0.5949 - val_loss: 0.6908\n",
      "Epoch 26/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6286 - loss: 0.6054 - val_accuracy: 0.5949 - val_loss: 0.6811\n",
      "Epoch 27/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6534 - loss: 0.5885 - val_accuracy: 0.5988 - val_loss: 0.6939\n",
      "Epoch 28/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6704 - loss: 0.5785 - val_accuracy: 0.6364 - val_loss: 0.6902\n",
      "Epoch 29/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6714 - loss: 0.5984 - val_accuracy: 0.5731 - val_loss: 0.6842\n",
      "Epoch 30/30\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6561 - loss: 0.5759 - val_accuracy: 0.5909 - val_loss: 0.6770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x219ea484b30>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_new, y_train_new, epochs=30, batch_size=32, validation_data=(X_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Precision: 0.6049382716049383\n",
      "Recall: 0.708092485549133\n",
      "F1 Score: 0.6524633821571239\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred).astype(int)  # Convert probabilities to binary labels\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9592 - loss: 0.1083 - val_accuracy: 0.6067 - val_loss: 1.4608\n",
      "Epoch 2/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9654 - loss: 0.1056 - val_accuracy: 0.6047 - val_loss: 1.5023\n",
      "Epoch 3/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9618 - loss: 0.1068 - val_accuracy: 0.5850 - val_loss: 1.5111\n",
      "Epoch 4/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9659 - loss: 0.1011 - val_accuracy: 0.6186 - val_loss: 1.5026\n",
      "Epoch 5/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9658 - loss: 0.0915 - val_accuracy: 0.6186 - val_loss: 1.5407\n",
      "Epoch 6/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9572 - loss: 0.0983 - val_accuracy: 0.6186 - val_loss: 1.4450\n",
      "Epoch 7/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9552 - loss: 0.1094 - val_accuracy: 0.6028 - val_loss: 1.4515\n",
      "Epoch 8/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9652 - loss: 0.0937 - val_accuracy: 0.6087 - val_loss: 1.5211\n",
      "Epoch 9/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9657 - loss: 0.0932 - val_accuracy: 0.6107 - val_loss: 1.5578\n",
      "Epoch 10/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9647 - loss: 0.1030 - val_accuracy: 0.5988 - val_loss: 1.5567\n",
      "Epoch 11/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9651 - loss: 0.1015 - val_accuracy: 0.5870 - val_loss: 1.5445\n",
      "Epoch 12/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9653 - loss: 0.0942 - val_accuracy: 0.6126 - val_loss: 1.5694\n",
      "Epoch 13/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9679 - loss: 0.0931 - val_accuracy: 0.6186 - val_loss: 1.5737\n",
      "Epoch 14/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9682 - loss: 0.0849 - val_accuracy: 0.6245 - val_loss: 1.6212\n",
      "Epoch 15/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9744 - loss: 0.0727 - val_accuracy: 0.6324 - val_loss: 1.5860\n",
      "Epoch 16/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9667 - loss: 0.0922 - val_accuracy: 0.6146 - val_loss: 1.6266\n",
      "Epoch 17/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9682 - loss: 0.0772 - val_accuracy: 0.6146 - val_loss: 1.6094\n",
      "Epoch 18/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9693 - loss: 0.0832 - val_accuracy: 0.5968 - val_loss: 1.6281\n",
      "Epoch 19/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9623 - loss: 0.0865 - val_accuracy: 0.6126 - val_loss: 1.6386\n",
      "Epoch 20/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9626 - loss: 0.0815 - val_accuracy: 0.6245 - val_loss: 1.6530\n",
      "Epoch 21/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9737 - loss: 0.0796 - val_accuracy: 0.6126 - val_loss: 1.6419\n",
      "Epoch 22/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9625 - loss: 0.0936 - val_accuracy: 0.6107 - val_loss: 1.6709\n",
      "Epoch 23/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9535 - loss: 0.1126 - val_accuracy: 0.6146 - val_loss: 1.5831\n",
      "Epoch 24/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9637 - loss: 0.0856 - val_accuracy: 0.6344 - val_loss: 1.6772\n",
      "Epoch 25/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9674 - loss: 0.0876 - val_accuracy: 0.6206 - val_loss: 1.6729\n",
      "Epoch 26/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9573 - loss: 0.1120 - val_accuracy: 0.6087 - val_loss: 1.6041\n",
      "Epoch 27/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.0684 - val_accuracy: 0.6008 - val_loss: 1.6491\n",
      "Epoch 28/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9640 - loss: 0.1098 - val_accuracy: 0.5949 - val_loss: 1.5011\n",
      "Epoch 29/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9754 - loss: 0.0735 - val_accuracy: 0.6166 - val_loss: 1.5925\n",
      "Epoch 30/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9601 - loss: 0.1040 - val_accuracy: 0.6146 - val_loss: 1.5276\n",
      "Epoch 31/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9774 - loss: 0.0730 - val_accuracy: 0.6087 - val_loss: 1.5599\n",
      "Epoch 32/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9652 - loss: 0.0732 - val_accuracy: 0.6225 - val_loss: 1.6699\n",
      "Epoch 33/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9779 - loss: 0.0565 - val_accuracy: 0.6225 - val_loss: 1.6246\n",
      "Epoch 34/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9710 - loss: 0.0773 - val_accuracy: 0.5929 - val_loss: 1.6538\n",
      "Epoch 35/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9691 - loss: 0.0999 - val_accuracy: 0.6087 - val_loss: 1.6337\n",
      "Epoch 36/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9790 - loss: 0.0635 - val_accuracy: 0.5988 - val_loss: 1.7767\n",
      "Epoch 37/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9739 - loss: 0.0655 - val_accuracy: 0.6126 - val_loss: 1.7709\n",
      "Epoch 38/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9714 - loss: 0.0884 - val_accuracy: 0.6126 - val_loss: 1.7128\n",
      "Epoch 39/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9735 - loss: 0.0800 - val_accuracy: 0.6087 - val_loss: 1.7258\n",
      "Epoch 40/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9727 - loss: 0.0612 - val_accuracy: 0.6028 - val_loss: 1.7524\n",
      "Epoch 41/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9735 - loss: 0.0704 - val_accuracy: 0.6225 - val_loss: 1.6740\n",
      "Epoch 42/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9658 - loss: 0.0846 - val_accuracy: 0.6304 - val_loss: 1.7167\n",
      "Epoch 43/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9726 - loss: 0.0659 - val_accuracy: 0.6383 - val_loss: 1.5912\n",
      "Epoch 44/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9808 - loss: 0.0674 - val_accuracy: 0.6225 - val_loss: 1.6576\n",
      "Epoch 45/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9690 - loss: 0.0749 - val_accuracy: 0.6126 - val_loss: 1.7406\n",
      "Epoch 46/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9763 - loss: 0.0643 - val_accuracy: 0.6186 - val_loss: 1.8262\n",
      "Epoch 47/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9774 - loss: 0.0525 - val_accuracy: 0.6225 - val_loss: 1.8028\n",
      "Epoch 48/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9789 - loss: 0.0578 - val_accuracy: 0.6225 - val_loss: 1.7192\n",
      "Epoch 49/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9650 - loss: 0.1019 - val_accuracy: 0.6245 - val_loss: 1.7704\n",
      "Epoch 50/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9660 - loss: 0.0859 - val_accuracy: 0.6126 - val_loss: 1.7543\n",
      "Epoch 51/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9753 - loss: 0.0721 - val_accuracy: 0.6047 - val_loss: 1.7400\n",
      "Epoch 52/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9735 - loss: 0.0624 - val_accuracy: 0.6107 - val_loss: 1.7219\n",
      "Epoch 53/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9783 - loss: 0.0723 - val_accuracy: 0.6166 - val_loss: 1.6575\n",
      "Epoch 54/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9782 - loss: 0.0598 - val_accuracy: 0.6067 - val_loss: 1.7264\n",
      "Epoch 55/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9832 - loss: 0.0588 - val_accuracy: 0.6126 - val_loss: 1.8578\n",
      "Epoch 56/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9804 - loss: 0.0514 - val_accuracy: 0.5988 - val_loss: 1.9343\n",
      "Epoch 57/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9745 - loss: 0.0642 - val_accuracy: 0.6107 - val_loss: 1.9708\n",
      "Epoch 58/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9781 - loss: 0.0563 - val_accuracy: 0.6146 - val_loss: 1.8389\n",
      "Epoch 59/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9766 - loss: 0.0635 - val_accuracy: 0.5988 - val_loss: 1.7740\n",
      "Epoch 60/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9768 - loss: 0.0682 - val_accuracy: 0.6047 - val_loss: 1.7977\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Precision: 0.5850340136054422\n",
      "Recall: 0.7456647398843931\n",
      "F1 Score: 0.6556543837357052\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_scaled_new, y_train_new, epochs=60, batch_size=32, validation_data=(X_val_scaled, y_val), verbose=1)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred).astype(int)  # Convert probabilities to binary labels\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\songy\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">52</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1536</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">196,736</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m52\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_9 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │        \u001b[38;5;34m24,704\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling1d_10 (\u001b[38;5;33mMaxPooling1D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_11 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1536\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m196,736\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_12 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │           \u001b[38;5;34m129\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">222,593</span> (869.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m222,593\u001b[0m (869.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">222,209</span> (868.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m222,209\u001b[0m (868.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">384</span> (1.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m384\u001b[0m (1.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional layer\n",
    "model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Another convolutional layer\n",
    "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Flattening followed by dense layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))  \n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model Summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/120\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5906 - loss: 0.6594 - val_accuracy: 0.6126 - val_loss: 0.6531\n",
      "Epoch 2/120\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5946 - loss: 0.6577 - val_accuracy: 0.6245 - val_loss: 0.6528\n",
      "Epoch 3/120\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6126 - loss: 0.6532 - val_accuracy: 0.6304 - val_loss: 0.6529\n",
      "Epoch 4/120\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6344 - loss: 0.6404 - val_accuracy: 0.6265 - val_loss: 0.6532\n",
      "Epoch 5/120\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5965 - loss: 0.6565 - val_accuracy: 0.6285 - val_loss: 0.6532\n",
      "Epoch 6/120\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5883 - loss: 0.6559 - val_accuracy: 0.6344 - val_loss: 0.6504\n",
      "Epoch 7/120\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5959 - loss: 0.6589 - val_accuracy: 0.6304 - val_loss: 0.6555\n",
      "Epoch 8/120\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5909 - loss: 0.6663 - val_accuracy: 0.5988 - val_loss: 0.6601\n",
      "Epoch 9/120\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6100 - loss: 0.6488 - val_accuracy: 0.6265 - val_loss: 0.6506\n",
      "Epoch 10/120\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6061 - loss: 0.6552 - val_accuracy: 0.6364 - val_loss: 0.6555\n",
      "Epoch 11/120\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5874 - loss: 0.6622 - val_accuracy: 0.6423 - val_loss: 0.6543\n",
      "Epoch 12/120\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6107 - loss: 0.6505 - val_accuracy: 0.6206 - val_loss: 0.6546\n",
      "Epoch 13/120\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.5955 - loss: 0.6506 - val_accuracy: 0.5929 - val_loss: 0.6602\n",
      "Epoch 14/120\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5846 - loss: 0.6550 - val_accuracy: 0.6304 - val_loss: 0.6580\n",
      "Epoch 15/120\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6001 - loss: 0.6467 - val_accuracy: 0.6166 - val_loss: 0.6582\n",
      "Epoch 16/120\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6059 - loss: 0.6543 - val_accuracy: 0.6245 - val_loss: 0.6553\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x219f1b0e000>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_new, y_train_new, epochs=120, batch_size=32, validation_data=(X_val, y_val), verbose=1, callbacks=[early_stop])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "Precision: 0.5895196506550219\n",
      "Recall: 0.7803468208092486\n",
      "F1 Score: 0.6716417910447762\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred).astype(int)  # Convert probabilities to binary labels\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9694 - loss: 0.0718 - val_accuracy: 0.6047 - val_loss: 1.7975\n",
      "Epoch 2/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9641 - loss: 0.0897 - val_accuracy: 0.5988 - val_loss: 1.7267\n",
      "Epoch 3/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9651 - loss: 0.0788 - val_accuracy: 0.6028 - val_loss: 1.7965\n",
      "Epoch 4/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9718 - loss: 0.0694 - val_accuracy: 0.5771 - val_loss: 1.8422\n",
      "Epoch 5/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9722 - loss: 0.0653 - val_accuracy: 0.6166 - val_loss: 1.8579\n",
      "Epoch 6/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9748 - loss: 0.0720 - val_accuracy: 0.6265 - val_loss: 1.8573\n",
      "Epoch 7/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9723 - loss: 0.0712 - val_accuracy: 0.6186 - val_loss: 1.7482\n",
      "Epoch 8/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9789 - loss: 0.0519 - val_accuracy: 0.6087 - val_loss: 1.8075\n",
      "Epoch 9/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9825 - loss: 0.0619 - val_accuracy: 0.5929 - val_loss: 1.8274\n",
      "Epoch 10/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9781 - loss: 0.0505 - val_accuracy: 0.6285 - val_loss: 1.9168\n",
      "Epoch 11/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9783 - loss: 0.0542 - val_accuracy: 0.6166 - val_loss: 1.8445\n",
      "Epoch 12/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9733 - loss: 0.0660 - val_accuracy: 0.6146 - val_loss: 1.8434\n",
      "Epoch 13/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9792 - loss: 0.0631 - val_accuracy: 0.6166 - val_loss: 1.8547\n",
      "Epoch 14/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9803 - loss: 0.0516 - val_accuracy: 0.6166 - val_loss: 1.8925\n",
      "Epoch 15/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9840 - loss: 0.0505 - val_accuracy: 0.6067 - val_loss: 1.9777\n",
      "Epoch 16/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9773 - loss: 0.0642 - val_accuracy: 0.6166 - val_loss: 1.8790\n",
      "Epoch 17/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9863 - loss: 0.0379 - val_accuracy: 0.6107 - val_loss: 1.9078\n",
      "Epoch 18/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9829 - loss: 0.0589 - val_accuracy: 0.6344 - val_loss: 1.8885\n",
      "Epoch 19/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9785 - loss: 0.0555 - val_accuracy: 0.6225 - val_loss: 2.0311\n",
      "Epoch 20/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9745 - loss: 0.0589 - val_accuracy: 0.6285 - val_loss: 1.8431\n",
      "Epoch 21/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9789 - loss: 0.0472 - val_accuracy: 0.6304 - val_loss: 1.9017\n",
      "Epoch 22/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9777 - loss: 0.0782 - val_accuracy: 0.6265 - val_loss: 1.8548\n",
      "Epoch 23/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9744 - loss: 0.0590 - val_accuracy: 0.6423 - val_loss: 1.8378\n",
      "Epoch 24/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9758 - loss: 0.0573 - val_accuracy: 0.6344 - val_loss: 1.8110\n",
      "Epoch 25/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9832 - loss: 0.0434 - val_accuracy: 0.6166 - val_loss: 1.8357\n",
      "Epoch 26/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9819 - loss: 0.0559 - val_accuracy: 0.6107 - val_loss: 1.8932\n",
      "Epoch 27/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9845 - loss: 0.0429 - val_accuracy: 0.6225 - val_loss: 1.8666\n",
      "Epoch 28/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9812 - loss: 0.0441 - val_accuracy: 0.6087 - val_loss: 1.9908\n",
      "Epoch 29/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9826 - loss: 0.0499 - val_accuracy: 0.6324 - val_loss: 1.9015\n",
      "Epoch 30/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9839 - loss: 0.0544 - val_accuracy: 0.6008 - val_loss: 1.7600\n",
      "Epoch 31/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9776 - loss: 0.0655 - val_accuracy: 0.6324 - val_loss: 1.7679\n",
      "Epoch 32/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9770 - loss: 0.0664 - val_accuracy: 0.6383 - val_loss: 1.8410\n",
      "Epoch 33/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9840 - loss: 0.0463 - val_accuracy: 0.6285 - val_loss: 1.9297\n",
      "Epoch 34/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9833 - loss: 0.0530 - val_accuracy: 0.6364 - val_loss: 1.8059\n",
      "Epoch 35/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9816 - loss: 0.0534 - val_accuracy: 0.6403 - val_loss: 1.7664\n",
      "Epoch 36/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9759 - loss: 0.0499 - val_accuracy: 0.6265 - val_loss: 1.8647\n",
      "Epoch 37/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9790 - loss: 0.0593 - val_accuracy: 0.6087 - val_loss: 1.8390\n",
      "Epoch 38/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9797 - loss: 0.0545 - val_accuracy: 0.6107 - val_loss: 1.7633\n",
      "Epoch 39/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9916 - loss: 0.0385 - val_accuracy: 0.6225 - val_loss: 1.9990\n",
      "Epoch 40/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9835 - loss: 0.0416 - val_accuracy: 0.6304 - val_loss: 1.9401\n",
      "Epoch 41/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9827 - loss: 0.0443 - val_accuracy: 0.6285 - val_loss: 1.9079\n",
      "Epoch 42/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9805 - loss: 0.0490 - val_accuracy: 0.6324 - val_loss: 1.7216\n",
      "Epoch 43/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9844 - loss: 0.0460 - val_accuracy: 0.6344 - val_loss: 1.7635\n",
      "Epoch 44/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9831 - loss: 0.0448 - val_accuracy: 0.6383 - val_loss: 1.7942\n",
      "Epoch 45/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9804 - loss: 0.0522 - val_accuracy: 0.6304 - val_loss: 1.8210\n",
      "Epoch 46/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9836 - loss: 0.0449 - val_accuracy: 0.6285 - val_loss: 1.9182\n",
      "Epoch 47/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9847 - loss: 0.0623 - val_accuracy: 0.6344 - val_loss: 1.8182\n",
      "Epoch 48/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9756 - loss: 0.0628 - val_accuracy: 0.6146 - val_loss: 1.9339\n",
      "Epoch 49/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9839 - loss: 0.0453 - val_accuracy: 0.6146 - val_loss: 1.9266\n",
      "Epoch 50/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9768 - loss: 0.0501 - val_accuracy: 0.6245 - val_loss: 1.7653\n",
      "Epoch 51/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9767 - loss: 0.0510 - val_accuracy: 0.6285 - val_loss: 1.7751\n",
      "Epoch 52/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9815 - loss: 0.0438 - val_accuracy: 0.6146 - val_loss: 1.8439\n",
      "Epoch 53/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9814 - loss: 0.0562 - val_accuracy: 0.6107 - val_loss: 1.8281\n",
      "Epoch 54/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9847 - loss: 0.0441 - val_accuracy: 0.6067 - val_loss: 1.9413\n",
      "Epoch 55/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9776 - loss: 0.0659 - val_accuracy: 0.6166 - val_loss: 1.8714\n",
      "Epoch 56/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9786 - loss: 0.0526 - val_accuracy: 0.6067 - val_loss: 1.6823\n",
      "Epoch 57/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9790 - loss: 0.0665 - val_accuracy: 0.5988 - val_loss: 1.7109\n",
      "Epoch 58/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9815 - loss: 0.0573 - val_accuracy: 0.6107 - val_loss: 1.9010\n",
      "Epoch 59/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9817 - loss: 0.0582 - val_accuracy: 0.6146 - val_loss: 1.9424\n",
      "Epoch 60/60\n",
      "\u001b[1m64/64\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9821 - loss: 0.0469 - val_accuracy: 0.6107 - val_loss: 2.0504\n",
      "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "Precision: 0.6071428571428571\n",
      "Recall: 0.7369942196531792\n",
      "F1 Score: 0.6657963446475196\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train_scaled_new, y_train_new, epochs=60, batch_size=32, validation_data=(X_val_scaled, y_val), verbose=1)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.round(y_pred).astype(int)  # Convert probabilities to binary labels\n",
    "\n",
    "\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
