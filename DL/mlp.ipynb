{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "if \"pipeline\" not in os.listdir():\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "from pipeline.preprocessing import \\\n",
    "    build_feature_extractor, \\\n",
    "    TCDPdata, \\\n",
    "    gen_datesets, \\\n",
    "    cross_train\n",
    "\n",
    "# constant\n",
    "dataset_root = \"assets/the-circor-digiscope-phonocardiogram-dataset-1.0.3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "class args:\n",
    "    \n",
    "    cutoff_frequency = 2000 # use 0 to disable bandpass filter\n",
    "\n",
    "    use_features = [\n",
    "        \"chromagram\",\n",
    "        \"melspectrogram\",\n",
    "        \"mfcc\",\n",
    "        \"csv\"\n",
    "    ]\n",
    "\n",
    "    use_X = [\n",
    "        \"raw\",\n",
    "        \"scaled\",\n",
    "        \"minmax\"\n",
    "    ]\n",
    "\n",
    "    train_size = 0.8\n",
    "\n",
    "    random_state = 2024\n",
    "\n",
    "    use_models = {\n",
    "        \"MLP\": {\n",
    "            \"class\": MLPClassifier,\n",
    "            \"kwargs\": {\n",
    "                \"hidden_layer_sizes\": (\n",
    "                    150,\n",
    "                    150,\n",
    "                    100,\n",
    "                    100\n",
    "                ),\n",
    "                \"activation\": 'logistic',\n",
    "                \"max_iter\": 100,\n",
    "                \"random_state\": random_state\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "random.seed(args.random_state)\n",
    "np.random.seed(args.random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # pd.DataFrame(features).to_csv(\"./assets/feature.csv\", header=False, index=False)\n",
    "# _, labels = TCDPdata(dataset_root).getXy(lambda _: None)\n",
    "# features = np.array(pd.read_csv(\"./assets/feature.csv\", header=None))\n",
    "\n",
    "# assert len(labels) == 3159\n",
    "# assert sum(labels) == 1632\n",
    "# print('n features:', features.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3159 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3159/3159 [04:02<00:00, 13.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n features: 254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "features, labels = TCDPdata(dataset_root).getXy(build_feature_extractor(args.use_features, args.cutoff_frequency))\n",
    "\n",
    "assert len(labels) == 3159\n",
    "assert sum(labels) == 1632\n",
    "print('n features:', features.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw scaled minmax\n"
     ]
    }
   ],
   "source": [
    "X, y = gen_datesets(features, labels, args.use_X, args.train_size, args.random_state)\n",
    "print(*X.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In case you run the next cell accidently,\n",
    "which can make you lose all the data.\n",
    "You need to run the cell first before the next one.\n",
    "\"\"\"\n",
    "models = {}\n",
    "scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: raw, Model: MLP, Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on train set:\n",
      "{'accuracy': 0.999604273842501,\n",
      " 'auc': 0.9995874587458745,\n",
      " 'f1': 0.999619916381604}\n",
      "Performance on test set:\n",
      "{'accuracy': 0.6123417721518988,\n",
      " 'auc': 0.612222722948275,\n",
      " 'f1': 0.6270928462709284}\n",
      "\n",
      "Dataset: scaled, Model: MLP, Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance on train set:\n",
      "{'accuracy': 0.9928769291650178,\n",
      " 'auc': 0.9929943279499053,\n",
      " 'f1': 0.9931350114416476}\n",
      "Performance on test set:\n",
      "{'accuracy': 0.6503164556962026,\n",
      " 'auc': 0.6501777577487358,\n",
      " 'f1': 0.6656580937972769}\n",
      "\n",
      "Dataset: minmax, Model: MLP, Training...\n",
      "Performance on train set:\n",
      "{'accuracy': 0.6913335971507717,\n",
      " 'auc': 0.6839049304169962,\n",
      " 'f1': 0.7449313276651407}\n",
      "Performance on test set:\n",
      "{'accuracy': 0.6313291139240507,\n",
      " 'auc': 0.6306894997746734,\n",
      " 'f1': 0.6938239159001315}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assert len(scores) == len(models) == 0, \"rerun the cell above to start a new experiment\"\n",
    "assert len(args.use_X) > 0 and len(args.use_models) > 0, \"at least one pair of train/test sets and one model is required\"\n",
    "\n",
    "models, scores = cross_train(X, y, args.use_models)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
